Setting up FastHepSim..
HOST=atlaslogin01.hep.anl.gov atlaslogin01.hep.anl.gov 
HEP ANL ROOT setup
PROMC was set to /users/admin/share/sl7/promc
[1;35m -- HEP ANL software for CentOS7 x -- [0m


Start calculations with 100 output slices
Start calculations with 20 input slices
NN to freq. conversion 1e+06 slices
Max number of epoch 200
Read file: data/train.data
Nr of events in trained sample=939839
Nr of input nodes in the matrix=6
Shuffle: data/train.data
calculate min and max for differences
Min and Max values for input variables: 
n=0 min=25 4209.55
n=1 min=-2.99998 2.99996
n=2 min=-3.14158 3.14159
n=3 min=1.00018 654.092
n=4 min=2.20432e-06 1.13752
n=5 min=0 39.453

 -> Output file is =data/input.root
#####  Rec-true distribution:
  mean=8.83978
  RMS=19.3321
  RMS90=11.6365
  min and max -1082.5 - 968.133
  Used min and max -49.1564 66.836
  efficiency mean=0.781994
  Used step to bin resolution= 1.17164
  input layer:      120 units
  hidden layer 1:   120  units
  output layer:     100  units
Input layer                          : 120 neurons, 1 bias
  Hidden layer                       : 120 neurons, 1 bias
Output layer                         : 100 neurons
Total neurons and biases             : 342
Total connections                    :26620
Connection rate                      :   1.000
Network type                         :   FANN_NETTYPE_LAYER
Training algorithm                   :   FANN_TRAIN_RPROP
Training error function              :   FANN_ERRORFUNC_TANH
Training stop function               :   FANN_STOPFUNC_MSE
Bit fail limit                       :   0.350
Learning rate                        :   0.700
Learning momentum                    :   0.000
Quickprop decay                      :  -0.000100
Quickprop mu                         :   1.750
RPROP increase factor                :   1.200
RPROP decrease factor                :   0.500
RPROP delta min                      :   0.000
RPROP delta max                      :  50.000
Cascade output change fraction       :   0.010000
Cascade candidate change fraction    :   0.010000
Cascade output stagnation epochs     :  12
Cascade candidate stagnation epochs  :  12
Cascade max output epochs            : 150
Cascade min output epochs            :  50
Cascade max candidate epochs         : 150
Cascade min candidate epochs         :  50
Cascade weight multiplier            :   0.400
Cascade candidate limit              :1000.000
Cascade activation functions[0]      :   FANN_SIGMOID
Cascade activation functions[1]      :   FANN_SIGMOID_SYMMETRIC
Cascade activation functions[2]      :   FANN_GAUSSIAN
Cascade activation functions[3]      :   FANN_GAUSSIAN_SYMMETRIC
Cascade activation functions[4]      :   FANN_ELLIOT
Cascade activation functions[5]      :   FANN_ELLIOT_SYMMETRIC
Cascade activation functions[6]      :   FANN_SIN_SYMMETRIC
Cascade activation functions[7]      :   FANN_COS_SYMMETRIC
Cascade activation functions[8]      :   FANN_SIN
Cascade activation functions[9]      :   FANN_COS
Cascade activation steepnesses[0]    :   0.250
Cascade activation steepnesses[1]    :   0.500
Cascade activation steepnesses[2]    :   0.750
Cascade activation steepnesses[3]    :   1.000
Cascade candidate groups             :   2
Cascade no. of candidates            :  80
Total number of neurons=342
Total number of connections=26620
-> Training using 16 threads
Write output.d: p efficiency purity 
# epoch=1 MSE=0.178512 Delta=9999.02
# epoch=2 MSE=0.178512 Delta=9999.02
# epoch=3 MSE=0.178512 Delta=9999.02
# epoch=4 MSE=0.0155046 Delta=9999.07
# epoch=5 MSE=0.0453558 Delta=9999.98
# epoch=6 MSE=0.0158105 Delta=9999.95
# epoch=7 MSE=0.0121385 Delta=9999.98
# epoch=8 MSE=0.0309261 Delta=9999.99
# epoch=9 MSE=0.0252073 Delta=9999.97
# epoch=10 MSE=0.0151356 Delta=9999.97
# epoch=20 MSE=0.00922837 Delta=0.0052309
# epoch=30 MSE=0.00925893 Delta=-0.000197768
# epoch=40 MSE=0.00867972 Delta=8.44998e-05
# epoch=50 MSE=0.00861207 Delta=-0.000416646
    -> cross valid MSE=0.00861207 Last min valid MSE=10000 out of N checks=1
Save intermidiate result to nn_out/neural_50.net
# epoch=60 MSE=0.00857051 Delta=-0.000456015
# epoch=70 MSE=0.00858475 Delta=-0.000496354
# epoch=80 MSE=0.00856139 Delta=-0.000472182
# epoch=90 MSE=0.00855819 Delta=-0.000494188
# epoch=100 MSE=0.00922735 Delta=-0.000860504
    -> cross valid MSE=0.00922735 Last min valid MSE=0.00861207 out of N checks=2
Save intermidiate result to nn_out/neural_100.net
# epoch=110 MSE=0.00853866 Delta=-7.70902e-05
# epoch=120 MSE=0.00854217 Delta=-0.000595887
# epoch=130 MSE=0.00853481 Delta=-0.000489033
# epoch=140 MSE=0.0101335 Delta=-0.000494707
# epoch=150 MSE=0.00876709 Delta=0.00079274
    -> cross valid MSE=0.00876709 Last min valid MSE=0.00861207 out of N checks=3
Save intermidiate result to nn_out/neural_150.net
# epoch=160 MSE=0.0085332 Delta=-0.000258351
# epoch=170 MSE=0.00922137 Delta=-0.0011524
# epoch=180 MSE=0.00853923 Delta=9.12445e-05
# epoch=190 MSE=0.0085418 Delta=-0.000492322
# epoch=200 MSE=0.00857697 Delta=-0.000477956
    -> cross valid MSE=0.00857697 Last min valid MSE=0.00861207 out of N checks=4
Save intermidiate result to nn_out/neural_200.net
Final MSE error=0.00857697
End. Save to: nn_out/neural_final.net
TFile: name=data/input.root, title=Histogram file, option=CREATE
TH1.Print Name  = rec-true, Entries= 734948, Total sum= 734947
TH1.Print Name  = input_eff, Entries= 939839, Total sum= 939839
TH1.Print Name  = input_res, Entries= 93044061, Total sum= 724348
Writing ROOT file data/input.root


Start calculations with 100 output slices
Start calculations with 20 input slices
NN to freq. conversion 1e+06 slices
Max number of epoch 200
Read file: data/train.data
Nr of events in trained sample=939839
Nr of input nodes in the matrix=6
Shuffle: data/train.data
calculate min and max for differences
Min and Max values for input variables: 
n=0 min=25 4209.55
n=1 min=-2.99998 2.99996
n=2 min=-3.14158 3.14159
n=3 min=1.00018 654.092
n=4 min=2.20432e-06 1.13752
n=5 min=0 39.453

 -> Output file is =data/output.root
#####  Rec-true distribution:
  mean=8.83978
  RMS=19.3321
  RMS90=11.6365
  min and max -1082.5 - 968.133
  Used min and max -49.1564 66.836
  efficiency mean=0.781994
  Used step to bin resolution= 1.17164

Testing network: open nn_out/neural_final.net
Read test: data/test.data
event=0
event=10000
event=20000
event=30000
event=40000
event=50000
event=60000
event=70000
event=80000
event=90000
event=100000
event=110000
event=120000
event=130000
event=140000
event=150000
event=160000
event=170000
event=180000
event=190000
event=200000
event=210000
event=220000
event=230000
event=240000
event=250000
event=260000
event=270000
event=280000
event=290000
event=300000
event=310000
event=320000
event=330000
event=340000
event=350000
event=360000
event=370000
event=380000
event=390000
event=400000
event=410000
event=420000
event=430000
event=440000
event=450000
event=460000
event=470000
event=480000
event=490000
event=500000
event=510000
event=520000
event=530000
event=540000
event=550000
event=560000
event=570000
event=580000
event=590000
event=600000
event=610000
event=620000
event=630000
event=640000
event=650000
event=660000
event=670000
event=680000
event=690000
event=700000
event=710000
event=720000
event=730000
event=740000
event=750000
event=760000
event=770000
event=780000
event=790000
event=800000
event=810000
event=820000
event=830000
event=840000
event=850000
event=860000
event=870000
event=880000

#### Predicted Rec-true distribution:
  mean=9.11369 true=8.83978
  RMS=16.0538 true=19.3321
  RMS90=11.4768 true = 11.6365
  efficiency mean=0.721395 true=0.781994
TFile: name=data/output.root, title=Histogram file, option=CREATE
TH1.Print Name  = rec-true, Entries= 734948, Total sum= 734947
TH1.Print Name  = input_eff, Entries= 939839, Total sum= 939839
TH1.Print Name  = input_res, Entries= 0, Total sum= 0
TH1.Print Name  = nn_res, Entries= 87582726, Total sum= 1.36189e+12
TH1.Print Name  = nn_eff, Entries= 884674, Total sum= 884652
TH1.Print Name  = predicted_res, Entries= 637892, Total sum= 637892
Writing ROOT file data/output.root
